name: Deploy to GCS & Create Dataproc Cluster

on:
  push:
    branches:
      - main

jobs:
  deploy:
    runs-on: ubuntu-latest
    env:
      BUCKET_NAME: rakshaka-dataproc-bucket
      INIT_SCRIPT_LOCAL: dataproc_init.sh
      INIT_SCRIPT_GCS: spark-jars/dataproc_init.sh

    steps:
      - name: Code Checkout
        uses: actions/checkout@v2

      - name: Create JSON credentials file
        id: create-json-credentials
        uses: jsdaniell/create-json@v1.2.3
        with:
          name: "gcloud-service-key.json"
          json: ${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}

      - name: Install the gcloud CLI
        uses: google-github-actions/setup-gcloud@v1
        with:
          project_id: ${{ secrets.GCP_PROJECT_ID }}
          service_account_key: ${{ secrets.GCP_SERVICE_ACCOUNT_KEY }}
          export_default_credentials: true

      - name: Authenticate gcloud CLI explicitly
        run: |
          gcloud auth activate-service-account --key-file=gcloud-service-key.json
          gcloud config set project ${{ secrets.GCP_PROJECT_ID }}

      - name: Ensure service account has necessary IAM roles
        run: |
          SA_EMAIL=$(gcloud auth list --filter=status:ACTIVE --format="value(account)")
          PROJECT_ID=${{ secrets.GCP_PROJECT_ID }}

          gcloud projects add-iam-policy-binding "$PROJECT_ID" \
            --member="serviceAccount:$SA_EMAIL" \
            --role="roles/dataproc.admin" \
            --quiet

          gcloud projects add-iam-policy-binding "$PROJECT_ID" \
            --member="serviceAccount:$SA_EMAIL" \
            --role="roles/storage.admin" \
            --quiet

          gcloud projects add-iam-policy-binding "$PROJECT_ID" \
            --member="serviceAccount:$SA_EMAIL" \
            --role="roles/iam.serviceAccountUser" \
            --quiet

          gcloud projects add-iam-policy-binding "$PROJECT_ID" \
            --member="serviceAccount:$SA_EMAIL" \
            --role="roles/logging.admin" \
            --quiet

      - name: Create GCS bucket if not exists
        run: |
          if ! gsutil ls -b gs://${BUCKET_NAME} >/dev/null 2>&1; then
            echo "Creating GCS bucket: ${BUCKET_NAME}"
            gsutil mb -l us-central1 gs://${BUCKET_NAME}
          else
            echo "GCS bucket ${BUCKET_NAME} already exists."
          fi

      - name: Upload dataproc_init.sh to GCS
        run: |
          gsutil cp ${INIT_SCRIPT_LOCAL} gs://${BUCKET_NAME}/${INIT_SCRIPT_GCS}

      - name: Create Dataproc Cluster
        run: |
          gcloud dataproc clusters create rakshaka \
            --region=us-central1 \
            --zone=us-central1-a \
            --master-machine-type=e2-standard-2 \
            --master-boot-disk-size=50GB \
            --num-workers=3 \
            --worker-machine-type=e2-standard-2 \
            --worker-boot-disk-size=50GB \
            --image-version=2.1-debian11 \
            --initialization-actions=gs://${BUCKET_NAME}/${INIT_SCRIPT_GCS} \
            --optional-components=JUPYTER \
            --enable-component-gateway
